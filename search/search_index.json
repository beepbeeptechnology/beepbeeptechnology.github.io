{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview We help people automate everything related to data. We are for the people who don't necessarily want to continually learn new tools or platforms, who just want data that they can rely on. We have deep and current expertise in data engineering, analytics engineering, data science and cloud architecture so you don't (necessarily) have to. Our Philosophy Whether you need data to make decisions, assess opportunities, evaluate policy outcomes, validate experimental results, train machine learning models or assess return on investment, the underlying approaches to data and analytics engineering are the same. But the tools and techniques required to ensure that data is available, reliable and easy to use are increasingly complex, with steep learning curves and ongoing maintenance activities. We believe that you just want your data exactly where you need it, consistently and reliably. That's where we come in. Our Approach We take a modular, sequential approach to all aspects of data management, which makes it easier to design, build, deploy, monitor, maintain, quality assure and troubleshoot data pipelines. We understand that most people want complexity to be abstracted away. You just want to be able to get the data you want into the tools and places you need it. Your data needs to be: timely reliable accurate well structured In order to provide this, we build and maintain native tools to source, cleanse, aggregate, integrate and augment your data with statistical analyses, computation and data from external APIs. We also have tools and services to monitor your pipelines, alert to any issues via multiple channels and resources to fix any issues which arise.","title":"Overview"},{"location":"#overview","text":"We help people automate everything related to data. We are for the people who don't necessarily want to continually learn new tools or platforms, who just want data that they can rely on. We have deep and current expertise in data engineering, analytics engineering, data science and cloud architecture so you don't (necessarily) have to.","title":"Overview"},{"location":"#our-philosophy","text":"Whether you need data to make decisions, assess opportunities, evaluate policy outcomes, validate experimental results, train machine learning models or assess return on investment, the underlying approaches to data and analytics engineering are the same. But the tools and techniques required to ensure that data is available, reliable and easy to use are increasingly complex, with steep learning curves and ongoing maintenance activities. We believe that you just want your data exactly where you need it, consistently and reliably. That's where we come in.","title":"Our Philosophy"},{"location":"#our-approach","text":"We take a modular, sequential approach to all aspects of data management, which makes it easier to design, build, deploy, monitor, maintain, quality assure and troubleshoot data pipelines. We understand that most people want complexity to be abstracted away. You just want to be able to get the data you want into the tools and places you need it. Your data needs to be: timely reliable accurate well structured In order to provide this, we build and maintain native tools to source, cleanse, aggregate, integrate and augment your data with statistical analyses, computation and data from external APIs. We also have tools and services to monitor your pipelines, alert to any issues via multiple channels and resources to fix any issues which arise.","title":"Our Approach"},{"location":"how_we_work/","text":"How we work We have a fully remote team based in the European Union, but with some flexibility to ensure that we can find overlap in terms of working hours with global clients. Our registered office is in Dublin, Ireland and we can invoice in EUR, USD, GBP or AUD. Initial Interest If you are interested in potentially using any of our services, please contact us at info@beepbeep.technology and we will get back to you within 48hrs to set up an initial consultation. This gives us the time to investigate any non-standard requirements in advance of an initial scoping call. Project Scoping We will then schedule an initial scoping video call to clarify any additional information and work through specific options available, as well as the likely trade-offs between up-front effort and ongoing management. We will also discuss options regarding infrastructure deployment and management, as well as any training requirements. Following this call we will be able to provide a high-level design outline and an indicative proposal. There will be more certainty over effort required for core services and we will highlight any areas where estimation uncertainty is higher. Operating Model You will also need to decide on the type of operating model under which you want to engage our services. This should be tailored to your individual needs, as well as the level of resources you want to apply to managing your data pipelines in future, and the capabilities of your internal resources. The type of model selected will impact the up-front and ongoing costs. Fully Managed In this approach we set all of your resources up on our managed infrastructure, and we take responsibility for ongoing resource management, quality assurance and ultimately data delivery. Your GCP project will only need to contain the BigQuery dataset and any other scoped Google destinations. As we develop and upgrade our resources, you will also have access to new functionality and tools. We will also take full responsibility for ongoing troubleshooting and pipeline fixes as and when they arise to ensure your data pipelines continue to function reliably. Cost Level Description Initial Cost Lower initial setup cost as we leverage existing operating infrastructure Ongoing Cost Higher ongoing cost as we manage and monitor resources and data pipelines Hybrid Model In this more flexible model, we can agree on which resources and processes you want to be managed by us, and what you want to manage in-house. The more resources we need to deploy on your GCP infrastructure for in-house management, the higher up-front cost but lower ongoing cost. You will also have access to new functionality and tools for any resource which remain on our infrastructure. We can also take responsibility for ongoing troubleshooting and pipeline fixes as and when they arise to ensure your data pipelines continue to function reliably. Cost Level Description Initial Cost Medium initial cost as we leverage some existing operating infrastructure Ongoing Cost Ongoing cost dependant on scope of monitoring/management services Self Managed In this approach we will clone our source code and deploy replicas of our resources onto your Google Cloud Platform project, and you will have full ownership rights over all components. We will provide you with tools and guidelines to monitor your data pipelines and you will have full responsibility for management, ongoing troubleshooting and pipeline fixes. If you require any additional services, this will be treated as a new engagement. You will not have access to new functionality and tools as we develop and upgrade our resources. Cost Level Description Initial Cost Higher initial cost as we sign over a snapshot of our code base Ongoing Cost No ongoing cost as you have full responsibility for monitoring/management Project Phases Project structure will be tailored to your specific needs, however it will typically involve some or all of the following phases. Design Following on from your initial brief and our subsequent scoping, we will draw up a high-level end-to-end design, comprising all of the data sources, destinations, components, refresh requirements, process responsibilities and other specifications unique to your requirements. Setup At this stage we set up the Google Cloud Platform project, set up access controls and generate credentials to interact with project resources. The setup here will vary depending on the comfort level you have with managing Google Cloud resources, and future operating model. Build Development of your data pipelines then takes place, with the GCP project dependant on your desired future operating model, with unit testing of the individual resources and integration testing of the end-to-end operation. Build is executed on the live architecture and project. Test Testing is an ongoing feature of our development model, and is not undertaken on a separate environment or separate project. Continuous testing of individual resources such as functions (unit tests) and end-to-end flows (integration tests) takes place as part of the build process. Deploy One of the major benefits of this development model is that once the build is completed, testing and deployment have already taken place onto the desired architecture, meaning that additional effort to deploy is minimal and your data pipelines will be up and running extremely quickly. Document We also produce reference documentation for your pipelines, with the format depending on the complexity of the pipelines. This forms a reference point for exactly what we have built and how it is connected, as well as a 'how-to' guide in terms of operations and troubleshooting. Train We can optionally provide training in various aspects of data engineering, analytics, data science and visualisation, with the depth and breadth depending on your precise objectives and the current/desired skills and capabilities of your team. Monitor Depending on the operating model, we can either set up the tools, templates and notifications to enable you to self-monitor your data pipelines, or we can leverage our existing in-house tools to monitor your data pipelines on your behalf. Maintain Similarly, depending on the operating model we can take care of all troubleshooting and ongoing maintenance on our in-house architecture, or we can hand over responsibility for ongoing maintenance and troubleshooting to your team. Next Steps Hopefully this information has given you a good idea on our data worldview, approach, scope of capabilities and the way we operate in collaboration with our clients. If you think you would like to explore partnering with us to help you automate your data then please get in touch with as much information as possible at info@beepbeep.technology , so we can review your requirements and send you a link to schedule an initial scoping call.","title":"How We Work"},{"location":"how_we_work/#how-we-work","text":"We have a fully remote team based in the European Union, but with some flexibility to ensure that we can find overlap in terms of working hours with global clients. Our registered office is in Dublin, Ireland and we can invoice in EUR, USD, GBP or AUD.","title":"How we work"},{"location":"how_we_work/#initial-interest","text":"If you are interested in potentially using any of our services, please contact us at info@beepbeep.technology and we will get back to you within 48hrs to set up an initial consultation. This gives us the time to investigate any non-standard requirements in advance of an initial scoping call.","title":"Initial Interest"},{"location":"how_we_work/#project-scoping","text":"We will then schedule an initial scoping video call to clarify any additional information and work through specific options available, as well as the likely trade-offs between up-front effort and ongoing management. We will also discuss options regarding infrastructure deployment and management, as well as any training requirements. Following this call we will be able to provide a high-level design outline and an indicative proposal. There will be more certainty over effort required for core services and we will highlight any areas where estimation uncertainty is higher.","title":"Project Scoping"},{"location":"how_we_work/#operating-model","text":"You will also need to decide on the type of operating model under which you want to engage our services. This should be tailored to your individual needs, as well as the level of resources you want to apply to managing your data pipelines in future, and the capabilities of your internal resources. The type of model selected will impact the up-front and ongoing costs. Fully Managed In this approach we set all of your resources up on our managed infrastructure, and we take responsibility for ongoing resource management, quality assurance and ultimately data delivery. Your GCP project will only need to contain the BigQuery dataset and any other scoped Google destinations. As we develop and upgrade our resources, you will also have access to new functionality and tools. We will also take full responsibility for ongoing troubleshooting and pipeline fixes as and when they arise to ensure your data pipelines continue to function reliably. Cost Level Description Initial Cost Lower initial setup cost as we leverage existing operating infrastructure Ongoing Cost Higher ongoing cost as we manage and monitor resources and data pipelines Hybrid Model In this more flexible model, we can agree on which resources and processes you want to be managed by us, and what you want to manage in-house. The more resources we need to deploy on your GCP infrastructure for in-house management, the higher up-front cost but lower ongoing cost. You will also have access to new functionality and tools for any resource which remain on our infrastructure. We can also take responsibility for ongoing troubleshooting and pipeline fixes as and when they arise to ensure your data pipelines continue to function reliably. Cost Level Description Initial Cost Medium initial cost as we leverage some existing operating infrastructure Ongoing Cost Ongoing cost dependant on scope of monitoring/management services Self Managed In this approach we will clone our source code and deploy replicas of our resources onto your Google Cloud Platform project, and you will have full ownership rights over all components. We will provide you with tools and guidelines to monitor your data pipelines and you will have full responsibility for management, ongoing troubleshooting and pipeline fixes. If you require any additional services, this will be treated as a new engagement. You will not have access to new functionality and tools as we develop and upgrade our resources. Cost Level Description Initial Cost Higher initial cost as we sign over a snapshot of our code base Ongoing Cost No ongoing cost as you have full responsibility for monitoring/management","title":"Operating Model"},{"location":"how_we_work/#project-phases","text":"Project structure will be tailored to your specific needs, however it will typically involve some or all of the following phases. Design Following on from your initial brief and our subsequent scoping, we will draw up a high-level end-to-end design, comprising all of the data sources, destinations, components, refresh requirements, process responsibilities and other specifications unique to your requirements. Setup At this stage we set up the Google Cloud Platform project, set up access controls and generate credentials to interact with project resources. The setup here will vary depending on the comfort level you have with managing Google Cloud resources, and future operating model. Build Development of your data pipelines then takes place, with the GCP project dependant on your desired future operating model, with unit testing of the individual resources and integration testing of the end-to-end operation. Build is executed on the live architecture and project. Test Testing is an ongoing feature of our development model, and is not undertaken on a separate environment or separate project. Continuous testing of individual resources such as functions (unit tests) and end-to-end flows (integration tests) takes place as part of the build process. Deploy One of the major benefits of this development model is that once the build is completed, testing and deployment have already taken place onto the desired architecture, meaning that additional effort to deploy is minimal and your data pipelines will be up and running extremely quickly. Document We also produce reference documentation for your pipelines, with the format depending on the complexity of the pipelines. This forms a reference point for exactly what we have built and how it is connected, as well as a 'how-to' guide in terms of operations and troubleshooting. Train We can optionally provide training in various aspects of data engineering, analytics, data science and visualisation, with the depth and breadth depending on your precise objectives and the current/desired skills and capabilities of your team. Monitor Depending on the operating model, we can either set up the tools, templates and notifications to enable you to self-monitor your data pipelines, or we can leverage our existing in-house tools to monitor your data pipelines on your behalf. Maintain Similarly, depending on the operating model we can take care of all troubleshooting and ongoing maintenance on our in-house architecture, or we can hand over responsibility for ongoing maintenance and troubleshooting to your team.","title":"Project Phases"},{"location":"how_we_work/#next-steps","text":"Hopefully this information has given you a good idea on our data worldview, approach, scope of capabilities and the way we operate in collaboration with our clients. If you think you would like to explore partnering with us to help you automate your data then please get in touch with as much information as possible at info@beepbeep.technology , so we can review your requirements and send you a link to schedule an initial scoping call.","title":"Next Steps"},{"location":"inbound/","text":"Inbound Data Automation The first step is determining where your data is and how you are going to get it into BigQuery. The approach depends on a number of factors, and you have a lot of options with varying degrees of up-front effort and ongoing maintenance and monitoring requirements. BigQuery Hosted If you are lucky enough then the data you need is already in BigQuery, and you don't need to do any inbound automation at all. This is great news. Public Datasets BigQuery Public Datasets are datasets which are hosted and maintained for free on BigQuery where you only pay query costs. You can explore these datasets through the BigQuery UI by browsing to the bigquery-public-datasets project. Automation Effort Level Initial Effort Ongoing Effort Commercial Datasets BigQuery Commercial Datasets are datasets which are hosted on BigQuery, but require you to enter into a commercial arrangement with the provider. Available datasets can be explored on the Google Cloud Marketplace by filtering for datasets. Automation Effort Level Initial Effort Ongoing Effort Google Managed Transfer The next best option is to let Google manage the transfer of data into BigQuery, which gives you two different mechanisms depending on the data source. Data Transfer Service If your data source is supported by the BigQuery Data Transfer Service , then you can configure a periodic transfer which is managed by Google. This includes a lot of Google data sources and Amazon S3 buckets, however with some limitations. Automation Effort Level Initial Effort Ongoing Effort Transfer Service for Cloud Data If your data is in an external storage bucket (i.e. Amazon S3 or Azure Storage Container) then you can use this service to transfer files (with more complex filtering functionality) to a Google Cloud Storage bucket and query this directly from BigQuery. Automation Effort Level Initial Effort Ongoing Effort Third Party Services There are a huge number of third-party services broadly known as ETL (Extract-Transform-Load) tools which will take care of managing your inbound data automation. These tools are constantly evolving, expanding and being acquired so we give a brief overview here which is accurate at the time of writing. The tool(s) you select will depend on the data sources which they support, which is always expanding. There is also another category of services which specialise in analysing customer behaviour, which will typically send data directly into BigQuery. Recommended ETL Tools We have had extensive exposure to these tools and deployed them to live data pipelines with a high level of success. As such we are comfortable setting up these tools on behalf of our clients, and either providing training to enable self-monitoring, or monitoring these services on their behalf. Automation Effort Level Initial Effort Ongoing Effort Stitch Data Stitch Data is a simple to set up, reliable and highly recommended option based on the Singer open standard. Unfortunately the previous generaous free tier has now been ' sunsetted '. Supermetrics Supermetrics is a marketing-focused service which can reliably push your data to a number of different places including BigQuery or direct into Google Data Studio. It also integrates natively with the Google Data Transfer Service (new options show up once you are signed up to the service) Rivery Rivery is a highly recommended service, with a huge number of additional capabilities and great monitoring tools. They will also build custom API integrations for a fee. Skyvia Skyvia is a service we have used with success as it offers some interesting integrations and customisation functionality. Data.World Data.World is a little different as it is technically a data catalog, but it has some interesting capabilities regarding inbound data automation and chart building. Meltano Meltano is actually an end-to-end open-source solution for more technically-minded people, developed by GitLab. It works by managing data transfer using Singer taps/targets, with DBT for data transformation and Apache Airflow for orchestration. Customer Analytics This set of services worth mentioning, as they give you the capability to track user behaviours across your website(s) and/or app(s). They require some technical knowledge to set up and manage, which is beyond the scope of our core services. However we are experienced at working with the data which they generate and push directly to BigQuery. Automation Effort Level Initial Effort Ongoing Effort Segment Mixpanel Heap Other ETL Tools There are a large number of other ETL tools out there, to which we have had limited - if any - exposure. These might suit your integration and/or budget requirements, but we do not have experience setting up or using the tools in a live environment. However, we are comfortable working with the data generated and sent to BigQuery. Automation Effort Level Initial Effort Ongoing Effort Fivetran Blendo Hevo Data XPlenty Alooma (acquired by Google) External Files We can automate the flow of data from files which are hosted publicly (open to anybody on the internet) or privately (behind a login or authentication mechanism). Complexity is driven by whether the URL and or filename is stable and consistent, and also whether the whole dataset is contained in the file or whether the data needs to be appended to the destination dataset. For robustness, our recommended approach is to periodically load the hosted data into our Cloud Storage, and build logic in the downstream pipeline to ensure that the destination dataset contains the correct and most recent data available. File types are typically CSV or JSON format. However we can also process Avro and Parquet files using existing processes, and potentailly additional file types upon request. Public Data In order to automate this process from a public website (e.g. a public GitHub respository) we use our own serverless architecture to periodically check for file changes and load new data when it becomes available. Automation Effort Level Initial Effort Ongoing Effort Private Data Accessing private data in external files is similar to accessing public data, and we apply the same checks for file change detection. However there is an additional layer of complexity as the request will need authentication credentials to access the data. Automation Effort Level Initial Effort Ongoing Effort External APIs A more complex but flexible and powerful source of data is via external APIs (Application Programming Interfaces), to which we send requests to periodically extract data, which we then manage and push to the downstream data pipeline. Complexity is driven by potentially changing response schemas and the need to orchestrate the requests so that no data is missed, changed data is updated and replicated data is de-duplicated downstream. We manage our custom API data sources using Serverless Cloud Functions in Python. Development of these is made more straightforward if there is a comprehensive client library. There is more significant development and management effort associated with custom API data sources, so we would advise you to use a third-party service if available. However, we are extremely experienced at API interaction so can build integrations and manage ongoing infrastructure for you if required as part of our core services. Public API Public APIs are the most straightforward to work with as there is no authentication mechanism, however there is still a base level of effort required to investigate, test, build and deploy any API data source. Automation Effort Level Initial Effort Ongoing Effort Authenticated API Authenticated APIs add an additionalty layer of complexity (and potential common point of failure) as the requests need to be authenticated before the API will send a response. As such there is additional initial effort and ongoing management. Automation Effort Level Initial Effort Ongoing Effort Multi-Tenant For some use cases you might need to access similar data sources for multiple clients, which comes with an even higher level of complexity due to more complex authntication processes (e.g. OAuth). Effort for these needs to be assessed on a case-by-case basis, but is generally high. Automation Effort Level Initial Effort Ongoing Effort","title":"Inbound Automation"},{"location":"inbound/#inbound-data-automation","text":"The first step is determining where your data is and how you are going to get it into BigQuery. The approach depends on a number of factors, and you have a lot of options with varying degrees of up-front effort and ongoing maintenance and monitoring requirements. BigQuery Hosted If you are lucky enough then the data you need is already in BigQuery, and you don't need to do any inbound automation at all. This is great news. Public Datasets BigQuery Public Datasets are datasets which are hosted and maintained for free on BigQuery where you only pay query costs. You can explore these datasets through the BigQuery UI by browsing to the bigquery-public-datasets project. Automation Effort Level Initial Effort Ongoing Effort Commercial Datasets BigQuery Commercial Datasets are datasets which are hosted on BigQuery, but require you to enter into a commercial arrangement with the provider. Available datasets can be explored on the Google Cloud Marketplace by filtering for datasets. Automation Effort Level Initial Effort Ongoing Effort Google Managed Transfer The next best option is to let Google manage the transfer of data into BigQuery, which gives you two different mechanisms depending on the data source. Data Transfer Service If your data source is supported by the BigQuery Data Transfer Service , then you can configure a periodic transfer which is managed by Google. This includes a lot of Google data sources and Amazon S3 buckets, however with some limitations. Automation Effort Level Initial Effort Ongoing Effort Transfer Service for Cloud Data If your data is in an external storage bucket (i.e. Amazon S3 or Azure Storage Container) then you can use this service to transfer files (with more complex filtering functionality) to a Google Cloud Storage bucket and query this directly from BigQuery. Automation Effort Level Initial Effort Ongoing Effort Third Party Services There are a huge number of third-party services broadly known as ETL (Extract-Transform-Load) tools which will take care of managing your inbound data automation. These tools are constantly evolving, expanding and being acquired so we give a brief overview here which is accurate at the time of writing. The tool(s) you select will depend on the data sources which they support, which is always expanding. There is also another category of services which specialise in analysing customer behaviour, which will typically send data directly into BigQuery. Recommended ETL Tools We have had extensive exposure to these tools and deployed them to live data pipelines with a high level of success. As such we are comfortable setting up these tools on behalf of our clients, and either providing training to enable self-monitoring, or monitoring these services on their behalf. Automation Effort Level Initial Effort Ongoing Effort Stitch Data Stitch Data is a simple to set up, reliable and highly recommended option based on the Singer open standard. Unfortunately the previous generaous free tier has now been ' sunsetted '. Supermetrics Supermetrics is a marketing-focused service which can reliably push your data to a number of different places including BigQuery or direct into Google Data Studio. It also integrates natively with the Google Data Transfer Service (new options show up once you are signed up to the service) Rivery Rivery is a highly recommended service, with a huge number of additional capabilities and great monitoring tools. They will also build custom API integrations for a fee. Skyvia Skyvia is a service we have used with success as it offers some interesting integrations and customisation functionality. Data.World Data.World is a little different as it is technically a data catalog, but it has some interesting capabilities regarding inbound data automation and chart building. Meltano Meltano is actually an end-to-end open-source solution for more technically-minded people, developed by GitLab. It works by managing data transfer using Singer taps/targets, with DBT for data transformation and Apache Airflow for orchestration. Customer Analytics This set of services worth mentioning, as they give you the capability to track user behaviours across your website(s) and/or app(s). They require some technical knowledge to set up and manage, which is beyond the scope of our core services. However we are experienced at working with the data which they generate and push directly to BigQuery. Automation Effort Level Initial Effort Ongoing Effort Segment Mixpanel Heap Other ETL Tools There are a large number of other ETL tools out there, to which we have had limited - if any - exposure. These might suit your integration and/or budget requirements, but we do not have experience setting up or using the tools in a live environment. However, we are comfortable working with the data generated and sent to BigQuery. Automation Effort Level Initial Effort Ongoing Effort Fivetran Blendo Hevo Data XPlenty Alooma (acquired by Google) External Files We can automate the flow of data from files which are hosted publicly (open to anybody on the internet) or privately (behind a login or authentication mechanism). Complexity is driven by whether the URL and or filename is stable and consistent, and also whether the whole dataset is contained in the file or whether the data needs to be appended to the destination dataset. For robustness, our recommended approach is to periodically load the hosted data into our Cloud Storage, and build logic in the downstream pipeline to ensure that the destination dataset contains the correct and most recent data available. File types are typically CSV or JSON format. However we can also process Avro and Parquet files using existing processes, and potentailly additional file types upon request. Public Data In order to automate this process from a public website (e.g. a public GitHub respository) we use our own serverless architecture to periodically check for file changes and load new data when it becomes available. Automation Effort Level Initial Effort Ongoing Effort Private Data Accessing private data in external files is similar to accessing public data, and we apply the same checks for file change detection. However there is an additional layer of complexity as the request will need authentication credentials to access the data. Automation Effort Level Initial Effort Ongoing Effort External APIs A more complex but flexible and powerful source of data is via external APIs (Application Programming Interfaces), to which we send requests to periodically extract data, which we then manage and push to the downstream data pipeline. Complexity is driven by potentially changing response schemas and the need to orchestrate the requests so that no data is missed, changed data is updated and replicated data is de-duplicated downstream. We manage our custom API data sources using Serverless Cloud Functions in Python. Development of these is made more straightforward if there is a comprehensive client library. There is more significant development and management effort associated with custom API data sources, so we would advise you to use a third-party service if available. However, we are extremely experienced at API interaction so can build integrations and manage ongoing infrastructure for you if required as part of our core services. Public API Public APIs are the most straightforward to work with as there is no authentication mechanism, however there is still a base level of effort required to investigate, test, build and deploy any API data source. Automation Effort Level Initial Effort Ongoing Effort Authenticated API Authenticated APIs add an additionalty layer of complexity (and potential common point of failure) as the requests need to be authenticated before the API will send a response. As such there is additional initial effort and ongoing management. Automation Effort Level Initial Effort Ongoing Effort Multi-Tenant For some use cases you might need to access similar data sources for multiple clients, which comes with an even higher level of complexity due to more complex authntication processes (e.g. OAuth). Effort for these needs to be assessed on a case-by-case basis, but is generally high. Automation Effort Level Initial Effort Ongoing Effort","title":"Inbound Data Automation"},{"location":"outbound/","text":"Outbound Data Automation Transformed data is made available for downstream tools and services as BigQuery views (dynamic data which will auto-refresh when queried) and/or BigQuery tables (periodically refreshed and typically faster to query). The tool you want to use to connect to your data should have a native BigQuery connector. If it doesn't then you can optionally: Use a different tool Get the company which provides the tool to build a connector Build (or pay a developer to build) a custom connector Find a workaround (via e.g. Google Sheets) We typically advise option 1 (use a tool with a native connector) as all other options incur varying degrees of additional up-front cost and/or ongoing monitoring and maintenance, with higher potential for failure in the future. Data Destinations Of course, setting up your inbound data automation and fine-tuning your transformation pipelines to build beautiful and bomb-proof data pipelines is a total waste of time unless you actually do something with the data! The categories of tools which need to be connected to your data in BigQuery are typically: BI Tools BI (Business Intelligence) tools are a fairly broad category, with a broad range of feature sets, complexity, prices, modernity and hosting options, but fundamentally aligned around translating data into visual representations to help find insights. We present a few options we have had experience with, none of which is perfect but all have positive reasons to consider them. Google Data Studio Google Data Studio is our go-to BI tool. It is far from perfect and has its own quirks, but it is completely free, has a rock-solid integration with BigQuery and is improving in functionality at a rapid pace. It's fully cloud-based, sharing and access control is simple via the usual Google mechanisms, and you can schedule email reports to go out periodically. Connection to BigQuery is very simple, robust and outlined here . Streamlit.io Streamlit is our go-to tool for data exploration, visualisation and web app development. It is pure-Python, highly interactive and supports some best-in-class python libraries for data visualisation and mapping. Tableau Tableau was a trailblazer in this space, but the veteran is - in our opinion at least - looking a little more dinosaur-like in this cloud native world. It is at its core a very powerful desktop application, with options to publish workbooks and data sources publicly or privately. Licensing model can get expensive, a steep learning curve and complex to quality assure dashboards. Connection to BigQuery is simple (outlined here ), but you need to be careful of data type errors. Looker Looker has always been a highly capable tool which is however prohibitively expensive for most startups and small organisations. Acquired by Google, it will be interesting to see if this means any more affordable options will emerge. Pioneer in data modelling/transformation, undoubtable powerful but locks you into their vendor-specific LookML view of the world. Connection to BigQuery requires creation of a service account in GCP, outlined here . Mode Mode Analytics is a slightly different beast, as it integrates Python, R and data visualisation into a modern SQL-based UI. Fully cloud-based and very intuitive interface, however the licensing model is very opaque and can (apparently) be quite expensive. As an aside, they have an amazing, comprehensive and free SQL tutorial we frequently reference for foundational concepts. Connection to BigQuery requires creation of a service account in GCP, outlined here . Microsoft Power BI Power BI is a good option if you are already bought into the Microsoft ecosystem as it is apparently a powerful and affordable BI tool with good integration the rest of the Microsoft tools. Connection is fairly straightforward (outlined here ). Vega Lite Vega Lite isn't technically a BI tool, rather a high-level grammar of interactive graphics used in a large and growing number of tools. Charts are specified in JSON, interactive and highly portable (even embeddable in Data Studio), and the Python library Altair is one of the best. There is also an excellent GUI based Chart Builder in data.world which connects to BigQuery. Connection from BigQuery to data.world is via a service account, outlined here . Spreadsheet Applications Spreadsheets are the tools in which analysts have historically done most of their data work, and they are so flexible that they can be used for pretty much any conceivable purpose. Which is the problem. In our opinion, spreadsheets are extremely useful for hacking around with data for quick and dirty data discovery, however when used as part of a workflow they can rapidly get out of hand. Processes built around spreadsheets don't scale well and are prone to undetected errors. Cloud hosted spreadsheets have taken away complexity around version control, but they still represent a risky point of failure and introduce human error, as well as 'helpful' automatic actions like dropping zeroes from the front of telephone numbers or changing all of your perfect ISO compliant dates into integers. Google Sheets Google Sheets has very good integrations with BigQuery, which can be extremely helpful. You can query data in a Google Sheet directly from BigQuery , and changes are reflected immediately when queried. You can also send data back the other way using Connected Sheets , and schedule extracts into your workbooks. This requires a GSuite for Business account. However, regarding this workflow: moving data through a spreadsheet can seem like a good idea until it breaks and you have no debugging framework, or any idea where to start trying to fix it. Microsoft Excel MIcrosoft Excel can connect to BigQuery, but setup is not trivial. You need to install the Magnitude Simba ODBC driver for BigQuery , which apparently works on Windows, Linux and MacOS. We have been able to get the connector working on Windows, however if you do need this workflow, then it might be worth thinking about your end-to-end workflow and whether it's still fit-for-purpose given the technical tools and resources we now have at our disposal. Lotus 1-2-3 Ha ha only joking! One from the history books ... Cloud Storage It is a fairly common data exchange pattern to be required to send data files to a cloud storage location for a partner/client/customer to process downstream. We can set up the mechanism to automate this export to a range of common cloud destinations. Google Cloud Storage Google Cloud Storage buckets are used for inbound data and can also be used as a location to periodically export data depending on specific requirements. Amazon S3 Amazon S3 buckets can also be used as a location to periodically export data depending on specific requirements. This would require additional Amazon authentication details. Azure Storage Microsoft Azure Storage Locations can also be used as a location to periodically export data depending on requirements. This would require additional Microsoft authentication details. Google Drive Files can also be periodically exported to a Google Drive location periodically, depending on the specific requirements. Dropbox Files can also be periodically exported to a Dropbox directory periodically, depending on the specific requirements. Programmatic Access BigQuery has Google-developed and supported client libraries for all major languages, which make interacting with your dataset programmatically very straightforward. Authentication processes vary, but consistently require you to create a service account, add the required permissions, download the key (JSON file) and then use the path to the key file in your code. Cloud Functions Cloud Functions are serverless, event-driven functions which can be triggered via a variety of secure mechanisms. They can be written in Node.js, Python, Go or Java, so programmatic access is via the Client Library for the relevant language. Python We have used the Python Client Library extensively and find it simple and robust. Node.js Documentation on how to set up the Node.js Client Library is available here . Go Documentation on how to set up the Go Client Library is available here . Java Documentation on how to set up the Java Client Library is available here . Ruby Documentation on how to set up the Ruby Client Library is available here . C# Documentation on how to set up the C# Client Library is available here . PHP Please don't. But if you must, the documentation is here . Attachments There are also use-cases where partners/clients/customers require data files to be sent periodically as attachments for downstream processing. We can set up the mechanism to automate this via a variety of channels. Email Email is probably the most common use case, but does involve some setup complexity to ensure that the emails are correctly authorised and not identified as spam. Slack Slack is a more straightforward process to set up, requiring some basic authentication details for attachments to be periodically attached to messages in a specific channel.","title":"Outbound Automation"},{"location":"outbound/#outbound-data-automation","text":"Transformed data is made available for downstream tools and services as BigQuery views (dynamic data which will auto-refresh when queried) and/or BigQuery tables (periodically refreshed and typically faster to query). The tool you want to use to connect to your data should have a native BigQuery connector. If it doesn't then you can optionally: Use a different tool Get the company which provides the tool to build a connector Build (or pay a developer to build) a custom connector Find a workaround (via e.g. Google Sheets) We typically advise option 1 (use a tool with a native connector) as all other options incur varying degrees of additional up-front cost and/or ongoing monitoring and maintenance, with higher potential for failure in the future.","title":"Outbound Data Automation"},{"location":"outbound/#data-destinations","text":"Of course, setting up your inbound data automation and fine-tuning your transformation pipelines to build beautiful and bomb-proof data pipelines is a total waste of time unless you actually do something with the data! The categories of tools which need to be connected to your data in BigQuery are typically: BI Tools BI (Business Intelligence) tools are a fairly broad category, with a broad range of feature sets, complexity, prices, modernity and hosting options, but fundamentally aligned around translating data into visual representations to help find insights. We present a few options we have had experience with, none of which is perfect but all have positive reasons to consider them. Google Data Studio Google Data Studio is our go-to BI tool. It is far from perfect and has its own quirks, but it is completely free, has a rock-solid integration with BigQuery and is improving in functionality at a rapid pace. It's fully cloud-based, sharing and access control is simple via the usual Google mechanisms, and you can schedule email reports to go out periodically. Connection to BigQuery is very simple, robust and outlined here . Streamlit.io Streamlit is our go-to tool for data exploration, visualisation and web app development. It is pure-Python, highly interactive and supports some best-in-class python libraries for data visualisation and mapping. Tableau Tableau was a trailblazer in this space, but the veteran is - in our opinion at least - looking a little more dinosaur-like in this cloud native world. It is at its core a very powerful desktop application, with options to publish workbooks and data sources publicly or privately. Licensing model can get expensive, a steep learning curve and complex to quality assure dashboards. Connection to BigQuery is simple (outlined here ), but you need to be careful of data type errors. Looker Looker has always been a highly capable tool which is however prohibitively expensive for most startups and small organisations. Acquired by Google, it will be interesting to see if this means any more affordable options will emerge. Pioneer in data modelling/transformation, undoubtable powerful but locks you into their vendor-specific LookML view of the world. Connection to BigQuery requires creation of a service account in GCP, outlined here . Mode Mode Analytics is a slightly different beast, as it integrates Python, R and data visualisation into a modern SQL-based UI. Fully cloud-based and very intuitive interface, however the licensing model is very opaque and can (apparently) be quite expensive. As an aside, they have an amazing, comprehensive and free SQL tutorial we frequently reference for foundational concepts. Connection to BigQuery requires creation of a service account in GCP, outlined here . Microsoft Power BI Power BI is a good option if you are already bought into the Microsoft ecosystem as it is apparently a powerful and affordable BI tool with good integration the rest of the Microsoft tools. Connection is fairly straightforward (outlined here ). Vega Lite Vega Lite isn't technically a BI tool, rather a high-level grammar of interactive graphics used in a large and growing number of tools. Charts are specified in JSON, interactive and highly portable (even embeddable in Data Studio), and the Python library Altair is one of the best. There is also an excellent GUI based Chart Builder in data.world which connects to BigQuery. Connection from BigQuery to data.world is via a service account, outlined here . Spreadsheet Applications Spreadsheets are the tools in which analysts have historically done most of their data work, and they are so flexible that they can be used for pretty much any conceivable purpose. Which is the problem. In our opinion, spreadsheets are extremely useful for hacking around with data for quick and dirty data discovery, however when used as part of a workflow they can rapidly get out of hand. Processes built around spreadsheets don't scale well and are prone to undetected errors. Cloud hosted spreadsheets have taken away complexity around version control, but they still represent a risky point of failure and introduce human error, as well as 'helpful' automatic actions like dropping zeroes from the front of telephone numbers or changing all of your perfect ISO compliant dates into integers. Google Sheets Google Sheets has very good integrations with BigQuery, which can be extremely helpful. You can query data in a Google Sheet directly from BigQuery , and changes are reflected immediately when queried. You can also send data back the other way using Connected Sheets , and schedule extracts into your workbooks. This requires a GSuite for Business account. However, regarding this workflow: moving data through a spreadsheet can seem like a good idea until it breaks and you have no debugging framework, or any idea where to start trying to fix it. Microsoft Excel MIcrosoft Excel can connect to BigQuery, but setup is not trivial. You need to install the Magnitude Simba ODBC driver for BigQuery , which apparently works on Windows, Linux and MacOS. We have been able to get the connector working on Windows, however if you do need this workflow, then it might be worth thinking about your end-to-end workflow and whether it's still fit-for-purpose given the technical tools and resources we now have at our disposal. Lotus 1-2-3 Ha ha only joking! One from the history books ... Cloud Storage It is a fairly common data exchange pattern to be required to send data files to a cloud storage location for a partner/client/customer to process downstream. We can set up the mechanism to automate this export to a range of common cloud destinations. Google Cloud Storage Google Cloud Storage buckets are used for inbound data and can also be used as a location to periodically export data depending on specific requirements. Amazon S3 Amazon S3 buckets can also be used as a location to periodically export data depending on specific requirements. This would require additional Amazon authentication details. Azure Storage Microsoft Azure Storage Locations can also be used as a location to periodically export data depending on requirements. This would require additional Microsoft authentication details. Google Drive Files can also be periodically exported to a Google Drive location periodically, depending on the specific requirements. Dropbox Files can also be periodically exported to a Dropbox directory periodically, depending on the specific requirements. Programmatic Access BigQuery has Google-developed and supported client libraries for all major languages, which make interacting with your dataset programmatically very straightforward. Authentication processes vary, but consistently require you to create a service account, add the required permissions, download the key (JSON file) and then use the path to the key file in your code. Cloud Functions Cloud Functions are serverless, event-driven functions which can be triggered via a variety of secure mechanisms. They can be written in Node.js, Python, Go or Java, so programmatic access is via the Client Library for the relevant language. Python We have used the Python Client Library extensively and find it simple and robust. Node.js Documentation on how to set up the Node.js Client Library is available here . Go Documentation on how to set up the Go Client Library is available here . Java Documentation on how to set up the Java Client Library is available here . Ruby Documentation on how to set up the Ruby Client Library is available here . C# Documentation on how to set up the C# Client Library is available here . PHP Please don't. But if you must, the documentation is here . Attachments There are also use-cases where partners/clients/customers require data files to be sent periodically as attachments for downstream processing. We can set up the mechanism to automate this via a variety of channels. Email Email is probably the most common use case, but does involve some setup complexity to ensure that the emails are correctly authorised and not identified as spam. Slack Slack is a more straightforward process to set up, requiring some basic authentication details for attachments to be periodically attached to messages in a specific channel.","title":"Data Destinations"},{"location":"transformation/","text":"Data Transformation Once your data is loaded into BigQuery, If your inbound data is structured, named and integrated in exactly the way you need to consume it, then that's awesome news! You can skip most of these steps, however you should still profile the data to ensure that it is free of e.g. duplicate rows or meaningless columns. Data Profiling Before undertaking any data transformation, it is important to profile the incoming data to understand the overall shape (schema, row count) and data types. Subsequently, investigation of the actual data is advisable to identify if the data contains duplicates, unexpected gaps, data type discontinuities or other characteristics which flag issues to address. It is also often extremely valuable to understand more nuanced characteristics of the data, from basic statistics (mean, median, mode), to understanding column cardinality, distribution and correlations between columns. Transformation Types We think about data transformations in terms of the following categories, with a range of different objectives and corresponding approaches. Cleanse Data cleanse activities automate the 'correction' of data, whether that is filtering out duplicate rows, cleaning unwanted characters from strings which can cause downstream errors. Cleanse activities can use native functionality and/or some generic or case-specific custom functions. Convert To leverage the full capabilities of BigQuery, it is important to ensure that data types are correct, which can be complex depending on the precise format the data is received in. Date and time formats can pose particular challenges, as well as differing national standards. Filter Common transformation actions are to filter down unwanted or unused columns from the data - improving downstream performance - or to filter down the dataset based on a set combination of column values, to expose a subset of the data to downstream tools. Aggregate Aggregate functions then perform some kind of aggregation on the date, grouping the date by a set of dimensions and executing a computation on the corresponding rows. Example functions are MIN , MAX , SUM or COUNT and normally result in a different (lower) output row count. Compute Computation functions are more complex and are often based on Analytic Functions , and can achieve complex computational outputs in a single step such as row occurrence counts, or encapsulated in datafunctions to quickly compute e.g. grouped moving averages. Transform Transform functions fundamentally reshape the data, which may be desirable for downstream purposes. These functions perform actions like pivotting data to transform tidy data into a set of dimensions and columns containing statistics, or unpivotting to achieve the inverse transform. Integrate You will almost certainly want to integrate your data with data from other sources. In SQL, this typically means a type of JOIN, which connects data on a shared set of dimensions. It is critical to profile pre- and post-join datasets as it is very easy to unwittingly duplicate rows or lose data. Transformation Options If your data needs to be cleansed, transformed or integrated with other data, you have some options for how you can do this. SQL BigQuery uses Standard SQL (Structured Query Language) for all data manipulation, which is a SQL dialect with some powerful additional capabilities. Simple transformation steps can be written as Standard SQL views, which can then be queried directly from your downstream tools. datafunctions For more complex and/or repetitive transformation requirements it can be extremely painful and time-consuming to hand-write SQL. In these instances, we leverage dynamic SQL in BigQuery. This enables common SQL patterns to be abstracted into re-useable functions ( User Defined Functions and Stored Procedures ) which can be chained together to build complex, reuseable transformation patterns. Underpinning this approach is a library of functions which we build and maintain called datafunctions ( datafunctions.github.io ), which are callable directly in BigQuery using the syntax: datafunctions.functiontype.function([ARGUMENTS]) This means that you can build complex transformations without ever having to leave the BigQuery UI or learn a completely new platform. Transformations can be chained together and parameterised, so you can apply the same transformation pattern to a limitless number of datasources with minimal additional effort. Third Party Platforms There are some excellent platforms which bring software engineering best practices to the data engineering workflow, and which might be applicable to your use case. As external tools there will be some onboarding effort and a learning curve as well as ongoing maintenance effort and potential cost. However these tools are worth consideration. dbt Data Build Tool ( dbt ) is a open source platform with a hosted paid option, which allows anyone comfortable with SQL to own the data transformation work that happens between loading data into your warehouse and analyzing it. It is written in Python and includes advanced templating using Jinja, version-controlled automated documentation, testing, package management and a huge amount of additional functionality. DataForm DataForm is another highly regarded modern analytics engineering platform, with a very similar objective to dbt, and a focus on collaboration between data teams and a unified, robust approach to data modelling. Whereas dbt is Python-centric, DataForm leverages JavaScript, so your choice of tool might be influenced by team capabilities in these languages. ETL Tools Some ETL tools enable you to do data transformations to a varying degree before you load the data into BigQuery. However our preferred approach is to separate logical activities and use ETL tools to load the data in the original form (with maybe an additional load timestamp or other load metadata), transform in BigQuery, and then give your external tools access to the transformed dataset. This gives the additional benefit of making it simpler to swap out pipeline components with minimal additional effort, avoiding being 'locked in' to specific providers. Downstream Tools You can also undertake transformations within your downstream tools, which might seem easier at the time, but which can cause longer term issues. Building complex logic into e.g. a spreadsheet or your Business Intelligence tool can be very difficult to quality assure, replicate or scale. Building all of your transformation logic into BigQuery enables you to avoid these potential pitfalls, as well as eliminating 'lock in' to a specific tool, and ensuring that every tool you use to access your data has the same, single source of truth.","title":"Transformation"},{"location":"transformation/#data-transformation","text":"Once your data is loaded into BigQuery, If your inbound data is structured, named and integrated in exactly the way you need to consume it, then that's awesome news! You can skip most of these steps, however you should still profile the data to ensure that it is free of e.g. duplicate rows or meaningless columns.","title":"Data Transformation"},{"location":"transformation/#data-profiling","text":"Before undertaking any data transformation, it is important to profile the incoming data to understand the overall shape (schema, row count) and data types. Subsequently, investigation of the actual data is advisable to identify if the data contains duplicates, unexpected gaps, data type discontinuities or other characteristics which flag issues to address. It is also often extremely valuable to understand more nuanced characteristics of the data, from basic statistics (mean, median, mode), to understanding column cardinality, distribution and correlations between columns.","title":"Data Profiling"},{"location":"transformation/#transformation-types","text":"We think about data transformations in terms of the following categories, with a range of different objectives and corresponding approaches. Cleanse Data cleanse activities automate the 'correction' of data, whether that is filtering out duplicate rows, cleaning unwanted characters from strings which can cause downstream errors. Cleanse activities can use native functionality and/or some generic or case-specific custom functions. Convert To leverage the full capabilities of BigQuery, it is important to ensure that data types are correct, which can be complex depending on the precise format the data is received in. Date and time formats can pose particular challenges, as well as differing national standards. Filter Common transformation actions are to filter down unwanted or unused columns from the data - improving downstream performance - or to filter down the dataset based on a set combination of column values, to expose a subset of the data to downstream tools. Aggregate Aggregate functions then perform some kind of aggregation on the date, grouping the date by a set of dimensions and executing a computation on the corresponding rows. Example functions are MIN , MAX , SUM or COUNT and normally result in a different (lower) output row count. Compute Computation functions are more complex and are often based on Analytic Functions , and can achieve complex computational outputs in a single step such as row occurrence counts, or encapsulated in datafunctions to quickly compute e.g. grouped moving averages. Transform Transform functions fundamentally reshape the data, which may be desirable for downstream purposes. These functions perform actions like pivotting data to transform tidy data into a set of dimensions and columns containing statistics, or unpivotting to achieve the inverse transform. Integrate You will almost certainly want to integrate your data with data from other sources. In SQL, this typically means a type of JOIN, which connects data on a shared set of dimensions. It is critical to profile pre- and post-join datasets as it is very easy to unwittingly duplicate rows or lose data.","title":"Transformation Types"},{"location":"transformation/#transformation-options","text":"If your data needs to be cleansed, transformed or integrated with other data, you have some options for how you can do this. SQL BigQuery uses Standard SQL (Structured Query Language) for all data manipulation, which is a SQL dialect with some powerful additional capabilities. Simple transformation steps can be written as Standard SQL views, which can then be queried directly from your downstream tools. datafunctions For more complex and/or repetitive transformation requirements it can be extremely painful and time-consuming to hand-write SQL. In these instances, we leverage dynamic SQL in BigQuery. This enables common SQL patterns to be abstracted into re-useable functions ( User Defined Functions and Stored Procedures ) which can be chained together to build complex, reuseable transformation patterns. Underpinning this approach is a library of functions which we build and maintain called datafunctions ( datafunctions.github.io ), which are callable directly in BigQuery using the syntax: datafunctions.functiontype.function([ARGUMENTS]) This means that you can build complex transformations without ever having to leave the BigQuery UI or learn a completely new platform. Transformations can be chained together and parameterised, so you can apply the same transformation pattern to a limitless number of datasources with minimal additional effort. Third Party Platforms There are some excellent platforms which bring software engineering best practices to the data engineering workflow, and which might be applicable to your use case. As external tools there will be some onboarding effort and a learning curve as well as ongoing maintenance effort and potential cost. However these tools are worth consideration. dbt Data Build Tool ( dbt ) is a open source platform with a hosted paid option, which allows anyone comfortable with SQL to own the data transformation work that happens between loading data into your warehouse and analyzing it. It is written in Python and includes advanced templating using Jinja, version-controlled automated documentation, testing, package management and a huge amount of additional functionality. DataForm DataForm is another highly regarded modern analytics engineering platform, with a very similar objective to dbt, and a focus on collaboration between data teams and a unified, robust approach to data modelling. Whereas dbt is Python-centric, DataForm leverages JavaScript, so your choice of tool might be influenced by team capabilities in these languages. ETL Tools Some ETL tools enable you to do data transformations to a varying degree before you load the data into BigQuery. However our preferred approach is to separate logical activities and use ETL tools to load the data in the original form (with maybe an additional load timestamp or other load metadata), transform in BigQuery, and then give your external tools access to the transformed dataset. This gives the additional benefit of making it simpler to swap out pipeline components with minimal additional effort, avoiding being 'locked in' to specific providers. Downstream Tools You can also undertake transformations within your downstream tools, which might seem easier at the time, but which can cause longer term issues. Building complex logic into e.g. a spreadsheet or your Business Intelligence tool can be very difficult to quality assure, replicate or scale. Building all of your transformation logic into BigQuery enables you to avoid these potential pitfalls, as well as eliminating 'lock in' to a specific tool, and ensuring that every tool you use to access your data has the same, single source of truth.","title":"Transformation Options"},{"location":"what_we_build/","text":"What We Build We build and monitor serverless, automated data pipelines on Google Cloud Platform, enabling you to rely on your data being exactly where you need it, when you need it and transformed exactly how you need it. Data Pipeline Overview Data pipelines comprise the following sequential actions which require coordination, execution and monitoring: Inbound Data Automation Data Transformation Outbound Data Automation Google BigQuery The most fundamental, foundational component of each data pipeline is your Data Warehouse. We exclusively use Google BigQuery due to its scaleability, ease of setup and use, powerful functionality set and pace of new feature release. If you have a Google Cloud Platform account (set one up here if not), then BigQuery is already available at console.cloud.google.com/bigquery . It is good to think of BigQuery as an external brain (built by some of the smartest people in the world), which can manage all of your data complexity, which scales to infinity and in most use cases is completely free of charge. It is completely serverless, which means that you never have to think about turning it on/off, managing underlying infrastructure or anything except using it. It is, in a word, amazing. Our worldview (from a data perspective, anyway) is consciously and unapologetically BigQuery-centric. Using BigQuery as the foundation, we then design, build, connect, deploy and manage your data pipeline components.","title":"What We Build"},{"location":"what_we_build/#what-we-build","text":"We build and monitor serverless, automated data pipelines on Google Cloud Platform, enabling you to rely on your data being exactly where you need it, when you need it and transformed exactly how you need it.","title":"What We Build"},{"location":"what_we_build/#data-pipeline-overview","text":"Data pipelines comprise the following sequential actions which require coordination, execution and monitoring: Inbound Data Automation Data Transformation Outbound Data Automation","title":"Data Pipeline Overview"},{"location":"what_we_build/#google-bigquery","text":"The most fundamental, foundational component of each data pipeline is your Data Warehouse. We exclusively use Google BigQuery due to its scaleability, ease of setup and use, powerful functionality set and pace of new feature release. If you have a Google Cloud Platform account (set one up here if not), then BigQuery is already available at console.cloud.google.com/bigquery . It is good to think of BigQuery as an external brain (built by some of the smartest people in the world), which can manage all of your data complexity, which scales to infinity and in most use cases is completely free of charge. It is completely serverless, which means that you never have to think about turning it on/off, managing underlying infrastructure or anything except using it. It is, in a word, amazing. Our worldview (from a data perspective, anyway) is consciously and unapologetically BigQuery-centric. Using BigQuery as the foundation, we then design, build, connect, deploy and manage your data pipeline components.","title":"Google BigQuery"}]}